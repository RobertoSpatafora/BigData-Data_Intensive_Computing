# BigData - Data Intensive Computing
Project within the Data Intensive Computing course at KTH. The aim of the project was to handle large amount of data, trying to extract some knowledge about them.

This project work based on analysis and visualization of a dataset containing Covid-19 related Twitter posts from April 2020. 
The dataset used has been obtained by merging two alreadybig public datasets, so obtaining a âˆ¼5GB dataset to work with:
- https://www.kaggle.com/smid80/coronavirus-covid19-tweets-early-april
- https://www.kaggle.com/smid80/coronavirus-covid19-tweets-late-april
Ananalysis has been conducted in order to understand whether the number of tweet re-posts are somehow correlated to the length of the tweet itself.  This might be interesting in order to understand whether posts length might influence their actual spread, helping the writer to reach more people. Note that the number of retweet is directly proportional to the spread of the tweet itself.  The spreadof tweets has been crucial in comforting people during hard pandemic times. The visualization part of the  project consists,  mainly,  in  two  different  aspects: correlation chart and wordcloud(s) (done with https://wordart.com/). The former is meant as a way to easily visualize the results obtained through the analysis of the correlation between post length and the number of retweets. The latter is agraphical representation of the most used words, giving to the reader an immediate idea about the most discussed topics.
